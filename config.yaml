# Local Ollama Agent Configuration

# Ollama model name (must be downloaded locally)
model: "llama3.2"

# System prompt for the agent
system_prompt: |
  You are a helpful AI assistant that can plan and execute tasks.
  Analyze tasks carefully, break them down into steps, and execute them systematically.
  Provide clear progress updates and handle errors gracefully.

# List of tasks to plan and execute
tasks:

# Optional Ollama API settings
settings:
  temperature: 0.7
  max_tokens: 2048
  # Ollama API endpoint (default: http://localhost:11434)
  api_endpoint: "http://localhost:11434"

# Embedding configuration for semantic knowledge base
embeddings:
  # Embedding model (must be downloaded locally: ollama pull nomic-embed-text)
  model: "nomic-embed-text"
  # Embedding dimension (nomic-embed-text uses 768)
  dimension: 768
  # Number of top results to retrieve in semantic search
  top_k: 10
  # Time decay factor for weighting recent interactions (0-1, higher = less decay)
  time_decay_factor: 0.95
  # Size of embedding cache (number of embeddings to keep in memory)
  cache_size: 1000

