# Local Ollama Agent Configuration

# Ollama model name (must be downloaded locally)
model: "llama3.2"

# System prompt for the agent
system_prompt: |
  You are a helpful AI assistant. Be concise. Plan and execute tasks systematically. 
  Provide clear, brief updates. Handle errors gracefully.

# List of tasks to plan and execute
tasks:

# Tools configuration (optional)
# Specify which tools this agent can use. If not specified, all tools are available.
# Available tools: write_file, read_file, list_directory
# Example:
# tools:
#   - write_file
#   - read_file
#   - list_directory
# To restrict tools, remove any you don't want the agent to have access to.
# For a read-only agent:
# tools:
#   - read_file
#   - list_directory

# Optional Ollama API settings
settings:
  temperature: 0.7
  max_tokens: 2048
  # Ollama API endpoint (default: http://localhost:11434)
  api_endpoint: "http://localhost:11434"

# Embedding configuration for semantic knowledge base
embeddings:
  # Embedding model (must be downloaded locally: ollama pull nomic-embed-text)
  model: "nomic-embed-text"
  # Embedding dimension (nomic-embed-text uses 768)
  dimension: 768
  # Number of top results to retrieve in semantic search
  top_k: 10
  # Time decay factor for weighting recent interactions (0-1, higher = less decay)
  time_decay_factor: 0.95
  # Size of embedding cache (number of embeddings to keep in memory)
  cache_size: 1000

